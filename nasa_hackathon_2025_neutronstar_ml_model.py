# -*- coding: utf-8 -*-
"""Nasa_Hackathon_2025_NeutronStar_ML_Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g9uSIRDPYXrSmVnUtKcaro47haVJuYOO
"""

#Objective: Create a ML model that is trained on one or more of NASA’s open-source exoplanet datasets to identify new exoplanets.
#Includes a web interface to facilitate user interaction.

import pandas as pd
from sklearn.metrics import roc_auc_score
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV
import joblib
from google.colab import files
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import LabelBinarizer
from google.colab import files
import json

#Preprocessing

df = pd.read_csv('Kepler_Data1.csv') #Loaded the Kepler dataset.
df.drop(columns=["loc_rowid", "koi_pdisposition", "koi_ingress", "koi_score",
                 "koi_fpflag_nt", "koi_fpflag_ss", "koi_fpflag_co", "koi_fpflag_ec"], inplace=True) #Human-given labels are removed to prevent data leakage.
df['koi_bin_oedp_sig'] = df.groupby('koi_disposition')['koi_bin_oedp_sig'].transform(
    lambda x: x.fillna(x.mean())
) #Column had about 16% of its data missing, filled with the mean values of the corresponding classes.
df.dropna(inplace=True) #Overall about 4% of the rows are dropped, the majority having over 50% of their data missing.

X = df.drop(columns=['koi_disposition'])
y = df['koi_disposition'] #Features and labels are separated.

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=30) #The dataset is split into train and test sets.

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)   #Encoded integers to the labels.

#Mapping:
for i, class_name in enumerate(le.classes_):
    print(f"{i} → {class_name}")

#Training

#The XGBoost algorithm is chosen as it gave the highest AUC score over a total of 8 models tested, including ensembles.
xgb_params = {
    'n_estimators': [200, 400, 600],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.03, 0.1, 0.3],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9],
    'reg_alpha': [0, 0.01, 0.1],
    'reg_lambda': [0.8, 1, 1.5]
}


xgb_base = xgb.XGBClassifier(
    objective='multi:softprob',   # Gives the probability distribution over classes.
    eval_metric='mlogloss',       # Penalizes the model more if it’s confidently wrong.
    num_class=3,
    use_label_encoder=False,
    enable_categorical=False,
    n_jobs=-1
)


xgb_search = RandomizedSearchCV(
    estimator=xgb_base,
    param_distributions=xgb_params,
    n_iter=30,  # Try 30 random combinations
    cv=3,       # 3-fold cross-validation
    scoring='roc_auc_ovr_weighted',
    n_jobs=-1,
    random_state=30,
    verbose=1
)


xgb_search.fit(X_train, y_train_enc)

# Got the best model and parameters:
best_xgb = xgb_search.best_estimator_
best_xgb_params = xgb_search.best_params_
best_xgb_score = xgb_search.best_score_


print("Best Parameters:", best_xgb_params)
print(f"Best CV AUC: {best_xgb_score:.3f}")

#Execution time of training: 6 minutes with default Colab settings

# Convert y_test to one-hot encoding:
lb = LabelBinarizer()
y_test_onehot = lb.fit_transform(y_test_enc)

y_prob_xgb = best_xgb.predict_proba(X_test)

# Overall weighted AUC:
xgb_auc = roc_auc_score(y_test_onehot, y_prob_xgb, average='weighted', multi_class='ovr')
print(f"XGBOOST TEST AUC: {xgb_auc:.3f}")


class_names = ['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE']

# Class-wise AUC:
for i, class_name in enumerate(class_names):
    class_auc = roc_auc_score(y_test_onehot[:, i], y_prob_xgb[:, i])
    print(f"{class_name}: {class_auc:.3f}")

# Get predictions for the test subset:
preds = best_xgb.predict(X_test)

# Original class names in order of their encoding:
class_names = ['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE']

print(classification_report(y_test_enc, preds, target_names=class_names))

# Save the model:
joblib.dump(best_xgb, 'exoplanet_classifier_model.pkl')

feature_info = {
    'feature_names': X_train.columns.tolist(),
    'class_names': best_xgb.classes_.tolist()
}

with open('model_features.json', 'w') as f:
    json.dump(feature_info, f)



# Download to your computer:
files.download('exoplanet_classifier_model.pkl')
files.download('model_features.json')